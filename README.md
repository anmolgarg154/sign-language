Sign Language Detection Project

📌 Project Overview
This project aims to develop an AI-based sign language detection system capable of recognizing and interpreting hand gestures. The system leverages computer vision and deep learning techniques to convert sign language into readable text, enabling better communication for individuals with hearing or speech impairments.

🚀 Features
Real-time Gesture Detection: Detects and classifies hand gestures using a webcam or video feed.

Flask Web Interface: Allows users to interact with the model through a web browser.

Multi-language Support: Recognizes gestures for multiple sign languages (e.g., ASL, ISL, BSL).

Text Output: Displays detected gestures as text in the web interface.

Model Training: Supports training custom models with labeled gesture datasets.

🛠️ Technologies Used

Backend: Flask (Python)

Frontend: HTML, CSS, JavaScript

Libraries:

OpenCV – Image processing and video feed handling

cvzone – Hand tracking and classification modules

NumPy – Data processing

TensorFlow – Deep learning model training and inference

Flask – Web framework

Base64 – Image encoding/decoding for web compatibility

Dataset: Pre-trained or custom datasets with labeled hand gestures


👥 Contributors- Anmol Garg